{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c261e5f4-17a8-40da-beb9-599f1717e0fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. 下载模型到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fba6e1f1-d9f6-4c9e-8e30-928dbe34b398",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded plugins: dkms-build-requires, extras_suggestions, kernel-livepatch,\n",
      "              : langpacks, priorities, update-motd, versionlock\n",
      "Existing lock /var/run/yum.pid: another copy is running as pid 30478.\n",
      "Another app is currently holding the yum lock; waiting for it to exit...\n",
      "  The other application is: yum\n",
      "    Memory : 173 M RSS (570 MB VSZ)\n",
      "    Started: Mon Dec 18 13:23:09 2023 - 00:03 ago\n",
      "    State  : Running, pid: 30478\n",
      "Existing lock /var/run/yum.pid: another copy is running as pid 30578.\n",
      "Another app is currently holding the yum lock; waiting for it to exit...\n",
      "  The other application is: yum\n",
      "    Memory : 115 M RSS (582 MB VSZ)\n",
      "    Started: Mon Dec 18 13:23:13 2023 - 00:01 ago\n",
      "    State  : Running, pid: 30578\n",
      "Another app is currently holding the yum lock; waiting for it to exit...\n",
      "  The other application is: yum\n",
      "    Memory : 195 M RSS (661 MB VSZ)\n",
      "    Started: Mon Dec 18 13:23:13 2023 - 00:03 ago\n",
      "    State  : Running, pid: 30578\n",
      "https://download.docker.com/linux/centos/2/x86_64/stable/repodata/repomd.xml: [Errno 14] HTTPS Error 404 - Not Found\n",
      "Trying other mirror.\n",
      "neuron                                                   | 2.9 kB     00:00     \n",
      "288 packages excluded due to repository priority protections\n",
      "No packages marked for update\n",
      "Loaded plugins: dkms-build-requires, extras_suggestions, kernel-livepatch,\n",
      "              : langpacks, priorities, update-motd, versionlock\n",
      "https://download.docker.com/linux/centos/2/x86_64/stable/repodata/repomd.xml: [Errno 14] HTTPS Error 404 - Not Found\n",
      "Trying other mirror.\n",
      "neuron                                                   | 2.9 kB     00:00     \n",
      "288 packages excluded due to repository priority protections\n",
      "Package amazon-linux-extras-2.0.3-1.amzn2.noarch already installed and latest version\n",
      "Nothing to do\n",
      "Installing epel-release\n",
      "Loaded plugins: dkms-build-requires, extras_suggestions, kernel-livepatch,\n",
      "              : langpacks, priorities, update-motd, versionlock\n",
      "Cleaning repos: amzn2-core amzn2extra-docker amzn2extra-epel\n",
      "              : amzn2extra-kernel-5.10 amzn2extra-livepatch amzn2extra-python3.8\n",
      "              : centos-extras\n",
      "              : copr:copr.fedorainfracloud.org:vbatts:shadow-utils-newxidmap\n",
      "              : docker-ce-stable epel libnvidia-container neuron\n",
      "45 metadata files removed\n",
      "20 sqlite files removed\n",
      "0 metadata files removed\n",
      "Loaded plugins: dkms-build-requires, extras_suggestions, kernel-livepatch,\n",
      "              : langpacks, priorities, update-motd, versionlock\n",
      "amzn2-core                                               | 3.6 kB     00:00     \n",
      "amzn2extra-docker                                        | 2.9 kB     00:00     \n",
      "amzn2extra-epel                                          | 3.0 kB     00:00     \n",
      "amzn2extra-kernel-5.10                                   | 3.0 kB     00:00     \n",
      "amzn2extra-livepatch                                     | 2.9 kB     00:00     \n",
      "amzn2extra-python3.8                                     | 2.9 kB     00:00     \n",
      "centos-extras                                            | 2.9 kB     00:00     \n",
      "copr:copr.fedorainfracloud.org:vbatts:shadow-utils-newxi | 3.3 kB     00:00     \n",
      "https://download.docker.com/linux/centos/2/x86_64/stable/repodata/repomd.xml: [Errno 14] HTTPS Error 404 - Not Found\n",
      "Trying other mirror.\n",
      "epel/x86_64/metalink                                     |  23 kB     00:00     \n",
      "epel                                                     | 4.7 kB     00:00     \n",
      "libnvidia-container/x86_64/signature                     |  833 B     00:00     \n",
      "libnvidia-container/x86_64/signature                     | 2.1 kB     00:02 !!! \n",
      "neuron                                                   | 2.9 kB     00:00     \n",
      "(1/20): amzn2-core/2/x86_64/group_gz                       | 2.7 kB   00:00     \n",
      "(2/20): amzn2-core/2/x86_64/updateinfo                     | 759 kB   00:00     \n",
      "(3/20): amzn2extra-epel/2/x86_64/primary_db                | 1.8 kB   00:00     \n",
      "(4/20): amzn2extra-kernel-5.10/2/x86_64/updateinfo         |  42 kB   00:00     \n",
      "(5/20): amzn2extra-docker/2/x86_64/updateinfo              |  13 kB   00:00     \n",
      "(6/20): amzn2extra-epel/2/x86_64/updateinfo                |   76 B   00:00     \n",
      "(7/20): amzn2extra-docker/2/x86_64/primary_db              | 105 kB   00:00     \n",
      "(8/20): amzn2extra-livepatch/2/x86_64/updateinfo           |  18 kB   00:00     \n",
      "(9/20): amzn2extra-python3.8/2/x86_64/updateinfo           | 4.3 kB   00:00     \n",
      "(10/20): amzn2extra-livepatch/2/x86_64/primary_db          |  52 kB   00:00     \n",
      "(11/20): amzn2extra-python3.8/2/x86_64/primary             |  16 kB   00:00     \n",
      "(12/20): copr:copr.fedorainfracloud.org:vbatts:shadow-util | 6.1 kB   00:00     \n",
      "(13/20): centos-extras/primary_db                          | 250 kB   00:00     \n",
      "(14/20): epel/x86_64/group_gz                              |  99 kB   00:00     \n",
      "(15/20): amzn2extra-kernel-5.10/2/x86_64/primary_db        |  21 MB   00:00     \n",
      "(16/20): epel/x86_64/updateinfo                            | 1.0 MB   00:00     \n",
      "(17/20): epel/x86_64/primary_db                            | 7.0 MB   00:00     \n",
      "(18/20): libnvidia-container/x86_64/primary                |  32 kB   00:00     \n",
      "(19/20): neuron/primary_db                                 | 153 kB   00:00     \n",
      "(20/20): amzn2-core/2/x86_64/primary_db                    |  69 MB   00:00     \n",
      "libnvidia-container                                                     211/211\n",
      "288 packages excluded due to repository priority protections\n",
      "Package epel-release-7-11.noarch already installed and latest version\n",
      "Nothing to do\n",
      "  2  httpd_modules            available  \u001b[0m  [ =1.0  =stable ]\n",
      "  3  memcached1.5             available  \u001b[0m  \\\n",
      "        [ =1.5.1  =1.5.16  =1.5.17 ]\n",
      "  9  R3.4                     available  \u001b[0m  [ =3.4.3  =stable ]\n",
      " 10  rust1                    available  \u001b[0m  \\\n",
      "        [ =1.22.1  =1.26.0  =1.26.1  =1.27.2  =1.31.0  =1.38.0\n",
      "          =stable ]\n",
      " 18  libreoffice              available  \u001b[0m  \\\n",
      "        [ =5.0.6.2_15  =5.3.6.1  =stable ]\n",
      " 19  gimp                     available  \u001b[0m  [ =2.8.22 ]\n",
      " 20 †\u001b[94mdocker=latest            enabled    \u001b[0m  \\\n",
      "        [ =17.12.1  =18.03.1  =18.06.1  =18.09.9  =stable ]\n",
      " 21  mate-desktop1.x          available  \u001b[0m  \\\n",
      "        [ =1.19.0  =1.20.0  =stable ]\n",
      " 22  GraphicsMagick1.3        available  \u001b[0m  \\\n",
      "        [ =1.3.29  =1.3.32  =1.3.34  =stable ]\n",
      " 23 †tomcat8.5                available  \u001b[0m  \\\n",
      "        [ =8.5.31  =8.5.32  =8.5.38  =8.5.40  =8.5.42  =8.5.50\n",
      "          =stable ]\n",
      " 24  \u001b[94mepel=latest              enabled    \u001b[0m  [ =7.11  =stable ]\n",
      " 25  testing                  available  \u001b[0m  [ =1.0  =stable ]\n",
      " 26  ecs                      available  \u001b[0m  [ =stable ]\n",
      " 27 †corretto8                available  \u001b[0m  \\\n",
      "        [ =1.8.0_192  =1.8.0_202  =1.8.0_212  =1.8.0_222  =1.8.0_232\n",
      "          =1.8.0_242  =stable ]\n",
      " 32  lustre2.10               available  \u001b[0m  \\\n",
      "        [ =2.10.5  =2.10.8  =stable ]\n",
      " 33 †java-openjdk11           available  \u001b[0m  [ =11  =stable ]\n",
      " 34  lynis                    available  \u001b[0m  [ =stable ]\n",
      " 36  BCC                      available  \u001b[0m  [ =0.x  =stable ]\n",
      " 37  mono                     available  \u001b[0m  [ =5.x  =stable ]\n",
      " 38  nginx1                   available  \u001b[0m  [ =stable ]\n",
      " 40  mock                     available  \u001b[0m  [ =stable ]\n",
      " 43  \u001b[94mlivepatch=latest         enabled    \u001b[0m  [ =stable ]\n",
      " 44 †\u001b[94mpython3.8=latest         enabled    \u001b[0m  [ =stable ]\n",
      " 45  haproxy2                 available  \u001b[0m  [ =stable ]\n",
      " 46  collectd                 available  \u001b[0m  [ =stable ]\n",
      " 47  aws-nitro-enclaves-cli   available  \u001b[0m  [ =stable ]\n",
      " 48  R4                       available  \u001b[0m  [ =stable ]\n",
      "  _  kernel-5.4               available  \u001b[0m  [ =stable ]\n",
      " 50  selinux-ng               available  \u001b[0m  [ =stable ]\n",
      " 52  tomcat9                  available  \u001b[0m  [ =stable ]\n",
      " 53  unbound1.13              available  \u001b[0m  [ =stable ]\n",
      " 54 †mariadb10.5              available  \u001b[0m  [ =stable ]\n",
      " 55  \u001b[94mkernel-5.10=latest       enabled    \u001b[0m  [ =stable ]\n",
      " 56  redis6                   available  \u001b[0m  [ =stable ]\n",
      " 57 †ruby3.0                  available  \u001b[0m  [ =stable ]\n",
      " 58 †postgresql12             available  \u001b[0m  [ =stable ]\n",
      " 59 †postgresql13             available  \u001b[0m  [ =stable ]\n",
      " 60  mock2                    available  \u001b[0m  [ =stable ]\n",
      " 61  dnsmasq2.85              available  \u001b[0m  [ =stable ]\n",
      " 62  kernel-5.15              available  \u001b[0m  [ =stable ]\n",
      " 63 †postgresql14             available  \u001b[0m  [ =stable ]\n",
      " 64  firefox                  available  \u001b[0m  [ =stable ]\n",
      " 65  lustre                   available  \u001b[0m  [ =stable ]\n",
      " 66 †php8.1                   available  \u001b[0m  [ =stable ]\n",
      " 67  awscli1                  available  \u001b[0m  [ =stable ]\n",
      " 68 †php8.2                   available  \u001b[0m  [ =stable ]\n",
      " 69  dnsmasq                  available  \u001b[0m  [ =stable ]\n",
      " 70  unbound1.17              available  \u001b[0m  [ =stable ]\n",
      " 72  collectd-python3         available  \u001b[0m  [ =stable ]\n",
      "† Note on end-of-support. Use 'info' subcommand.\n",
      "Loaded plugins: dkms-build-requires, extras_suggestions, kernel-livepatch,\n",
      "              : langpacks, priorities, update-motd, versionlock\n",
      "https://download.docker.com/linux/centos/2/x86_64/stable/repodata/repomd.xml: [Errno 14] HTTPS Error 404 - Not Found\n",
      "Trying other mirror.\n",
      "neuron                                                   | 2.9 kB     00:00     \n",
      "288 packages excluded due to repository priority protections\n",
      "No packages marked for update\n",
      "Loaded plugins: dkms-build-requires, extras_suggestions, kernel-livepatch,\n",
      "              : langpacks, priorities, update-motd, versionlock\n",
      "https://download.docker.com/linux/centos/2/x86_64/stable/repodata/repomd.xml: [Errno 14] HTTPS Error 404 - Not Found\n",
      "Trying other mirror.\n",
      "neuron                                                   | 2.9 kB     00:00     \n",
      "288 packages excluded due to repository priority protections\n",
      "Package git-lfs-2.10.0-2.el7.x86_64 already installed and latest version\n",
      "Package git-2.40.1-1.amzn2.0.1.x86_64 already installed and latest version\n",
      "Nothing to do\n"
     ]
    }
   ],
   "source": [
    "# For notebook instances (Amazon Linux)\n",
    "!sudo yum update -y\n",
    "!sudo yum install amazon-linux-extras\n",
    "!sudo amazon-linux-extras install epel -y\n",
    "!sudo yum update -y\n",
    "!sudo yum install git-lfs git -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1df1c7e4-d22a-4088-ab70-418d751d0091",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.wisemodel.cn/ZhipuAI/chatglm3-6b.git\n",
      "Updated git hooks.\n",
      "Git LFS initialized.\n",
      "Cloning into 'chatglm3-6b'...\n",
      "remote: Enumerating objects: 71, done.\u001b[K\n",
      "remote: Total 71 (delta 0), reused 0 (delta 0), pack-reused 71\u001b[K\n",
      "Receiving objects: 100% (71/71), 37.17 KiB | 12.39 MiB/s, done.\n",
      "Resolving deltas: 100% (28/28), done.\n",
      "Filtering content: 100% (8/8), 11.63 GiB | 9.08 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "#下载模型snapshot到本地，需要25G空间\n",
    "#需大约15-30分钟时间，请耐心等待, 如果左侧大括号内还是[*]，就还在下载中，*变成任意数例如[3]就证明已完成\n",
    "\n",
    "from pathlib import Path\n",
    "local_model_path = Path(\"./chatglm3-6b\")\n",
    "local_model_path.mkdir(exist_ok=True)\n",
    "model_name = \"ZhipuAI/chatglm3-6b\"\n",
    "clone_path = f\"https://www.wisemodel.cn/{model_name}.git\"\n",
    "print(clone_path)\n",
    "\n",
    "!git lfs install\n",
    "!git clone $clone_path\n",
    "!cd ./chatglm3-6b && rm -rf .git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e889dcf-a093-497d-902f-f6f130b69c01",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. 把模型拷贝到S3为后续部署做准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ba78615-76a3-46ad-a757-bf8f71a802dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "model_snapshot_path: chatglm3-6b\n",
      "s3_location: s3://sagemaker-us-east-1-340636688520/llm_model/chatglm3-6b/\n",
      "s3_code_prefix: lmi_inference_code/chatglm3-6b\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "sagemaker_session_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "region = sagemaker_session._region_name\n",
    "account_id = sagemaker_session.account_id()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "s3_code_prefix = f\"lmi_inference_code/{model_name.split('/')[-1]}\"\n",
    "\n",
    "s3_location = f\"s3://{sagemaker_session_bucket}/llm_model/{model_name.split('/')[-1]}/\"\n",
    "\n",
    "#你也可以把local_model_path直接替换成你的模型路径，例\"model_snapshot_path=./chatglm3-6b\", 这个文件夹里需要包含config.json\n",
    "model_snapshot_path = local_model_path\n",
    "\n",
    "print(f\"model_snapshot_path: {model_snapshot_path}\")\n",
    "print(\"s3_location:\",s3_location)\n",
    "print(\"s3_code_prefix:\",s3_code_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62f2fc81-329d-4f41-84a7-15f056a1e4a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: chatglm3-6b/README.md to s3://sagemaker-us-east-1-340636688520/llm_model/chatglm3-6b/README.md\n",
      "upload: chatglm3-6b/MODEL_LICENSE to s3://sagemaker-us-east-1-340636688520/llm_model/chatglm3-6b/MODEL_LICENSE\n",
      "upload: chatglm3-6b/config.json to s3://sagemaker-us-east-1-340636688520/llm_model/chatglm3-6b/config.json\n",
      "upload: chatglm3-6b/.gitattributes to s3://sagemaker-us-east-1-340636688520/llm_model/chatglm3-6b/.gitattributes\n",
      "upload: chatglm3-6b/configuration_chatglm.py to s3://sagemaker-us-east-1-340636688520/llm_model/chatglm3-6b/configuration_chatglm.py\n",
      "upload: chatglm3-6b/modeling_chatglm.py to s3://sagemaker-us-east-1-340636688520/llm_model/chatglm3-6b/modeling_chatglm.py\n",
      "upload: chatglm3-6b/pytorch_model-00001-of-00007.bin to s3://sagemaker-us-east-1-340636688520/llm_model/chatglm3-6b/pytorch_model-00001-of-00007.bin\n",
      "upload: chatglm3-6b/pytorch_model-00005-of-00007.bin to s3://sagemaker-us-east-1-340636688520/llm_model/chatglm3-6b/pytorch_model-00005-of-00007.bin\n",
      "upload: chatglm3-6b/pytorch_model-00004-of-00007.bin to s3://sagemaker-us-east-1-340636688520/llm_model/chatglm3-6b/pytorch_model-00004-of-00007.bin\n",
      "upload: chatglm3-6b/pytorch_model.bin.index.json to s3://sagemaker-us-east-1-340636688520/llm_model/chatglm3-6b/pytorch_model.bin.index.json\n",
      "upload: chatglm3-6b/quantization.py to s3://sagemaker-us-east-1-340636688520/llm_model/chatglm3-6b/quantization.py\n",
      "upload: chatglm3-6b/tokenization_chatglm.py to s3://sagemaker-us-east-1-340636688520/llm_model/chatglm3-6b/tokenization_chatglm.py\n",
      "upload: chatglm3-6b/tokenizer.model to s3://sagemaker-us-east-1-340636688520/llm_model/chatglm3-6b/tokenizer.model\n",
      "upload: chatglm3-6b/tokenizer_config.json to s3://sagemaker-us-east-1-340636688520/llm_model/chatglm3-6b/tokenizer_config.json\n",
      "upload: chatglm3-6b/pytorch_model-00003-of-00007.bin to s3://sagemaker-us-east-1-340636688520/llm_model/chatglm3-6b/pytorch_model-00003-of-00007.bin\n",
      "upload: chatglm3-6b/pytorch_model-00002-of-00007.bin to s3://sagemaker-us-east-1-340636688520/llm_model/chatglm3-6b/pytorch_model-00002-of-00007.bin\n",
      "upload: chatglm3-6b/pytorch_model-00007-of-00007.bin to s3://sagemaker-us-east-1-340636688520/llm_model/chatglm3-6b/pytorch_model-00007-of-00007.bin\n",
      "upload: chatglm3-6b/pytorch_model-00006-of-00007.bin to s3://sagemaker-us-east-1-340636688520/llm_model/chatglm3-6b/pytorch_model-00006-of-00007.bin\n"
     ]
    }
   ],
   "source": [
    "#上传模型\n",
    "!aws s3 sync $model_snapshot_path $s3_location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696b70c3-90f1-4175-95bf-568bafbcd383",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. 模型部署准备（entrypoint脚本，容器镜像，服务配置）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f7c4277-4480-42c6-aee6-1fbcca94eb82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image going to be used is ---- > 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.24.0-deepspeed0.10.0-cu118\n"
     ]
    }
   ],
   "source": [
    "inference_image_uri = (\n",
    "    f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.24.0-deepspeed0.10.0-cu118\"\n",
    ")\n",
    "if \"cn-\" in region:\n",
    "    inference_image_uri = (\n",
    "        f\"727897471807.dkr.ecr.{region}.amazonaws.com.cn/djl-inference:0.24.0-deepspeed0.10.0-cu118\"\n",
    "    )\n",
    "\n",
    "\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d771bdb-11d2-45d2-9bef-face29221838",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p LLM_chatglm3_6b_deploy_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9a3df19-fc7d-4a68-aa90-9f278262b618",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting LLM_chatglm3_6b_deploy_code/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile LLM_chatglm3_6b_deploy_code/model.py\n",
    "from djl_python import Input, Output\n",
    "import torch\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "from transformers import pipeline, AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "import deepspeed\n",
    "\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "\n",
    "\n",
    "def load_model(properties):\n",
    "    tensor_parallel_degree = properties[\"tensor_parallel_degree\"]\n",
    "    model_location = properties['model_dir']\n",
    "    if \"model_id\" in properties:\n",
    "        model_location = properties['model_id']\n",
    "    logging.info(f\"Loading model in {model_location}\")\n",
    "    \n",
    "    \n",
    "    print('============================tokenizer====================')\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_location, trust_remote_code=True)\n",
    "    \n",
    "    print('============================model====================')\n",
    "    \n",
    "    model = AutoModel.from_pretrained(model_location, trust_remote_code=True).half().cuda()\n",
    "    model = model.eval()\n",
    "    \n",
    "    print('============================model loaded====================')\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "generator = None\n",
    "\n",
    "def stream_items(prompt,history, max_length, top_p, temperature):\n",
    "    global model, tokenizer\n",
    "    size = 0\n",
    "    response = \"\"\n",
    "    for response, history in model.stream_chat(tokenizer, prompt, history=history, max_length=max_length, top_p=top_p,\n",
    "                                               temperature=temperature):\n",
    "        this_response = response[size:]\n",
    "        history = [list(h) for h in history]\n",
    "        size = len(response)\n",
    "        stream_buffer = { \"outputs\":this_response}\n",
    "        yield stream_buffer\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    global model, tokenizer\n",
    "    if not model:\n",
    "        model, tokenizer = load_model(inputs.get_properties())\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        return None\n",
    "    data = inputs.get_as_json()\n",
    "    \n",
    "    \n",
    "        \n",
    "    #zpf\n",
    "    input_sentences = data[\"ask\"]\n",
    "    params={}\n",
    "    if \"parameters\" in data:\n",
    "        params = data[\"parameters\"]\n",
    "    elif \"temperature\" in data:\n",
    "        params = {\"temperature\": data[\"temperature\"]}\n",
    "    else:\n",
    "        params = {\"temperature\":0.01}\n",
    "    history=[]\n",
    "    if \"history\" in data:\n",
    "        history = data[\"history\"]\n",
    "    stream=False\n",
    "    if \"stream\" in data:\n",
    "        stream = data.get('stream')\n",
    "    print(f'input prompt:{input_sentences}')  \n",
    "    outputs = Output()\n",
    "    if stream:\n",
    "        outputs.add_property(\"content-type\", \"application/jsonlines\")\n",
    "        outputs.add_stream_content(stream_items(prompt=input_sentences,history=history,**params))\n",
    "    else:\n",
    "        input_sentences = data[\"ask\"]\n",
    "        response, history = model.chat(tokenizer, input_sentences, history=history)\n",
    "    \n",
    "        result = {\"answer\": response}\n",
    "        return outputs.add_as_json(result)    \n",
    "    \n",
    "    return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8996fe44-8e70-468b-abc1-38187cb33f4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting LLM_chatglm3_6b_deploy_code/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile LLM_chatglm3_6b_deploy_code/serving.properties\n",
    "engine=Python\n",
    "option.tensor_parallel_degree=1\n",
    "option.model_id=s3://sagemaker-us-west-2-687912291502/llm/models/chatglm3-6b/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f906d376-0ec3-4dcf-9109-8647285b80eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#将模型的s3路径更新到inference.py中\n",
    "!sed -i 's|option.model_id=.*|option.model_id={s3_location}|' LLM_chatglm3_6b_deploy_code/serving.properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b7e76c6-6dbc-47fc-9f47-4765c526ab76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting LLM_chatglm3_6b_deploy_code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile LLM_chatglm3_6b_deploy_code/requirements.txt\n",
    "-i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "transformers==4.30.2\n",
    "cpm_kernels\n",
    "protobuf\n",
    "mdtex2html\n",
    "sentencepiece\n",
    "accelerate>=0.17.1\n",
    "einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0ae6734a-aacd-410d-818d-0a962697c3c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM_chatglm3_6b_deploy_code/\n",
      "LLM_chatglm3_6b_deploy_code/requirements.txt\n",
      "LLM_chatglm3_6b_deploy_code/model.py\n",
      "LLM_chatglm3_6b_deploy_code/serving.properties\n"
     ]
    }
   ],
   "source": [
    "!rm model.tar.gz\n",
    "!cd LLM_chatglm3_6b_deploy_code && rm -rf \".ipynb_checkpoints\"\n",
    "!tar czvf model.tar.gz LLM_chatglm3_6b_deploy_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f77dc76-6d8c-4665-ba88-f03e887c136c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-east-1-340636688520/lmi_inference_code/chatglm3-6b/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_code_artifact = sagemaker_session.upload_data(\"model.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5853daa-b8a3-4485-8c0a-64bf83e93a18",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. 创建模型 & 创建endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ef974ca1-9638-45a8-9145-ea9d03b2b072",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dji-inference-chatglm3-6b\n",
      "Image going to be used is ---- > 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.24.0-deepspeed0.10.0-cu118\n",
      "Created Model: arn:aws:sagemaker:us-east-1:340636688520:model/dji-inference-chatglm3-6b\n",
      "{'EndpointConfigArn': 'arn:aws:sagemaker:us-east-1:340636688520:endpoint-config/dji-inference-chatglm3-6b', 'ResponseMetadata': {'RequestId': '891d8b15-4803-44e0-bba9-61596733cae8', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '891d8b15-4803-44e0-bba9-61596733cae8', 'content-type': 'application/x-amz-json-1.1', 'content-length': '106', 'date': 'Mon, 18 Dec 2023 15:01:55 GMT'}, 'RetryAttempts': 0}}\n",
      "Created Endpoint: arn:aws:sagemaker:us-east-1:340636688520:endpoint/dji-inference-chatglm3-6b\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "import boto3\n",
    "\n",
    "model_name = 'dji-inference-chatglm3-6b'\n",
    "print(model_name)\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": inference_image_uri,\n",
    "        \"ModelDataUrl\": s3_code_artifact\n",
    "    },\n",
    "    \n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")\n",
    "\n",
    "endpoint_config_name = model_name\n",
    "endpoint_name = model_name\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.g4dn.4xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            # \"VolumeSizeInGB\" : 400,\n",
    "            # \"ModelDataDownloadTimeoutInSeconds\": 2400,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 15*60,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(endpoint_config_response)\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1262e826-a810-401d-a5a9-f62febb24e5f",
   "metadata": {},
   "source": [
    "#### 持续检测模型部署进度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08969928-6b9e-4d9c-a033-a31f5f77bdfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bc94bc-a2da-4c14-adcd-b4872911c986",
   "metadata": {},
   "source": [
    "### 5. 测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56c02256-d2ba-4e16-9d2e-e2d18f07bafd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'疲倦是人体的一种自然反应,通常是由于身体需要休息和恢复所发出的信号。以下是一些可以帮助你缓解疲劳感的方法:\\n\\n1. 休息:找到一个安静的地方,尽可能地放松身心,让自己得到充足的睡眠。\\n\\n2. 运动:适当的运动可以促进血液循环,增强身体的免疫力,缓解疲劳感。\\n\\n3. 饮食:饮食健康,多吃蔬菜水果,少吃油腻食品,可以提供给身体足够的能量。\\n\\n4. 调整心态:保持乐观的心态,减轻压力,可以有效地缓解疲劳感。\\n\\n5. 娱乐活动:听音乐、看电影、读书等活动可以缓解疲劳,让你的身心得到放松。\\n\\n希望这些方法可以帮助你缓解疲劳感。'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "runtime= boto3.client('runtime.sagemaker')\n",
    "def chatglm(prompt):\n",
    "  def query_endpoint_with_json_payload(encoded_json):\n",
    "    response = runtime.invoke_endpoint(EndpointName=model_name, ContentType='application/json', Body=encoded_json)\n",
    "    return response\n",
    "\n",
    "  def parse_response_texts(query_response):\n",
    "      model_predictions = json.loads(query_response['Body'].read())\n",
    "      generated_text = model_predictions[\"answer\"]\n",
    "      return generated_text\n",
    "  payload = {\"ask\": prompt,\n",
    "             \"parameters\": {}}\n",
    "  query_response = query_endpoint_with_json_payload(json.dumps(payload).encode('utf-8'))\n",
    "  generated_texts = parse_response_texts(query_response)\n",
    "  return generated_texts\n",
    "\n",
    "chatglm('好累啊')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a62b04e-db2a-41d7-a22f-ffca881d9a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "endpoint_name = model_name\n",
    "prompts1 = \"\"\"\n",
    "你是MySQL的专家。给定一个输入问题，创建一个语法正确的MySQL查询语句。\n",
    "除非用户在问题中指定了要获得的特定数量的示例，否则使用LIMIT子句查询最多3个结果。您可以对结果进行排序，以返回数据库中信息量最大的数据。您必须仅查询回答问题所需的列。将每个列名用反引号（`）括起来，表示为分隔的标识符。\n",
    "请注意，仅可以使用在下面这些表中看到的列名，不要查询不存在的列。此外，还要注意哪个列在哪个表中。如果问题涉及”今天”，请注意使用CURDATE()函数获取当前日期.\n",
    "\n",
    "使用如下格式:\n",
    "Question: 具体的问题\n",
    "SQLQuery: 运行的sql语句\n",
    "SQLResult: SQLQuery运行的结果\n",
    "Answer: 最终的回答\n",
    "\n",
    "\n",
    "使用如下的表:\n",
    "CREATE TABLE customer (\n",
    "\tc_customer_sk INTEGER NOT NULL, \n",
    "\tc_customer_id CHAR(16) NOT NULL, \n",
    "\tc_current_cdemo_sk INTEGER, \n",
    "\tc_current_hdemo_sk INTEGER, \n",
    "\tc_current_addr_sk INTEGER, \n",
    "\tc_first_shipto_date_sk INTEGER, \n",
    "\tc_first_sales_date_sk INTEGER, \n",
    "\tc_salutation CHAR(10), \n",
    "\tc_first_name CHAR(20), \n",
    "\tc_last_name CHAR(30), \n",
    "\tc_preferred_cust_flag CHAR(1), \n",
    "\tc_birth_day INTEGER, \n",
    "\tc_birth_month INTEGER, \n",
    "\tc_birth_year INTEGER, \n",
    "\tc_birth_country VARCHAR(20), \n",
    "\tc_login CHAR(13), \n",
    "\tc_email_address CHAR(50), \n",
    "\tc_last_review_date CHAR(10), \n",
    "\tPRIMARY KEY (c_customer_sk)\n",
    ")ENGINE=InnoDB DEFAULT CHARSET=utf8\n",
    "\n",
    "\n",
    "CREATE TABLE web_sales (\n",
    "\tws_sold_date_sk INTEGER, \n",
    "\tws_sold_time_sk INTEGER, \n",
    "\tws_ship_date_sk INTEGER, \n",
    "\tws_item_sk INTEGER NOT NULL, \n",
    "\tws_bill_customer_sk INTEGER, \n",
    "\tws_bill_cdemo_sk INTEGER, \n",
    "\tws_bill_hdemo_sk INTEGER, \n",
    "\tws_bill_addr_sk INTEGER, \n",
    "\tws_ship_customer_sk INTEGER, \n",
    "\tws_ship_cdemo_sk INTEGER, \n",
    "\tws_ship_hdemo_sk INTEGER, \n",
    "\tws_ship_addr_sk INTEGER, \n",
    "\tws_web_page_sk INTEGER, \n",
    "\tws_web_site_sk INTEGER, \n",
    "\tws_ship_mode_sk INTEGER, \n",
    "\tws_warehouse_sk INTEGER, \n",
    "\tws_promo_sk INTEGER, \n",
    "\tws_order_number INTEGER NOT NULL, \n",
    "\tws_quantity INTEGER, \n",
    "\tws_wholesale_cost DECIMAL(7, 2), \n",
    "\tws_list_price DECIMAL(7, 2), \n",
    "\tws_sales_price DECIMAL(7, 2), \n",
    "\tws_ext_discount_amt DECIMAL(7, 2), \n",
    "\tws_ext_sales_price DECIMAL(7, 2), \n",
    "\tws_ext_wholesale_cost DECIMAL(7, 2), \n",
    "\tws_ext_list_price DECIMAL(7, 2), \n",
    "\tws_ext_tax DECIMAL(7, 2), \n",
    "\tws_coupon_amt DECIMAL(7, 2), \n",
    "\tws_ext_ship_cost DECIMAL(7, 2), \n",
    "\tws_net_paid DECIMAL(7, 2), \n",
    "\tws_net_paid_inc_tax DECIMAL(7, 2), \n",
    "\tws_net_paid_inc_ship DECIMAL(7, 2), \n",
    "\tws_net_paid_inc_ship_tax DECIMAL(7, 2), \n",
    "\tws_net_profit DECIMAL(7, 2), \n",
    "\tPRIMARY KEY (ws_item_sk, ws_order_number)\n",
    ")ENGINE=InnoDB DEFAULT CHARSET=utf8\n",
    "\n",
    "Question: 我需要知道销售报表中，下单金额最大的客户email地址\n",
    "\"\"\"\n",
    "\n",
    "prompts2=\"给我一个青海和甘肃旅游的路线，8天7晚\"\n",
    "prompts3=\"好累啊\"\n",
    "parameters={\n",
    "    \"do_sample\": False,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 1,\n",
    "    \"max_new_tokens\": 300,\n",
    "    \"repetition_penalty\": 1.03\n",
    "}\n",
    "response_model = smr_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            Body=json.dumps(\n",
    "            {\n",
    "                \"ask\": prompts1,\n",
    "                \"parameters\": parameters,\n",
    "            }\n",
    "            ),\n",
    "            ContentType=\"application/json\",\n",
    "        )\n",
    "\n",
    "response_model['Body'].read().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363646a2-786b-4112-ae92-b3dc8d002f91",
   "metadata": {},
   "source": [
    "### 流式输出测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "503a7187-70bf-4c6a-9315-41a087ce4cb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "\n",
    "class LineIterator:\n",
    "    \"\"\"\n",
    "    A helper class for parsing the InvokeEndpointWithResponseStream event stream. \n",
    "    \n",
    "    The output of the model will be in the following format:\n",
    "    ```\n",
    "    b'{\"outputs\": [\" a\"]}\\n'\n",
    "    b'{\"outputs\": [\" challenging\"]}\\n'\n",
    "    b'{\"outputs\": [\" problem\"]}\\n'\n",
    "    ...\n",
    "    ```\n",
    "    \n",
    "    While usually each PayloadPart event from the event stream will contain a byte array \n",
    "    with a full json, this is not guaranteed and some of the json objects may be split across\n",
    "    PayloadPart events. For example:\n",
    "    ```\n",
    "    {'PayloadPart': {'Bytes': b'{\"outputs\": '}}\n",
    "    {'PayloadPart': {'Bytes': b'[\" problem\"]}\\n'}}\n",
    "    ```\n",
    "    \n",
    "    This class accounts for this by concatenating bytes written via the 'write' function\n",
    "    and then exposing a method which will return lines (ending with a '\\n' character) within\n",
    "    the buffer via the 'readlines' function. It maintains the position of the last read \n",
    "    position to ensure that previous bytes are not exposed again. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.buff = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "        \n",
    "    def write(self, content):\n",
    "        self.buff.seek(0, io.SEEK_END)\n",
    "        self.buff.write(content)\n",
    "        \n",
    "    def readlines(self):\n",
    "        self.buff.seek(self.read_pos)\n",
    "        for line in self.buff.readlines():\n",
    "            if line[-1] != b'\\n':\n",
    "                self.read_pos += len(line)\n",
    "                yield line[:-1]\n",
    "                \n",
    "    def reset(self):\n",
    "        self.read_pos = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cdd3c9f1-6dcc-434e-87b4-3d416a075e2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Life is the characteristic that distinguishes physical entities with biological processes, such as growth, reproduction, and response to stimuli, from those without such processes. It is the condition that distinguishes living things from non-living things, such as inanimate matter. The exact definition of life is still a topic of debate among scientists and philosophers."
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "  \"max_length\": 1024,\n",
    "  \"temperature\": 0.5,\n",
    "  \"top_p\":0.9\n",
    "}\n",
    "body = {\"ask\": \"what is life\", \"parameters\": parameters,\"history\" : [],\"stream\":True}\n",
    "resp = smr_client.invoke_endpoint_with_response_stream(EndpointName=endpoint_name, Body=json.dumps(body), ContentType=\"application/json\")\n",
    "event_stream = resp['Body']\n",
    "\n",
    "scanner = LineIterator()\n",
    "for event in event_stream:\n",
    "    scanner.write(event['PayloadPart']['Bytes'])\n",
    "    for line in scanner.readlines():\n",
    "        try:\n",
    "            resp = json.loads(line)\n",
    "            # print(resp)\n",
    "            print(resp.get(\"outputs\")['outputs'], end='')\n",
    "        except Exception as e:\n",
    "            # print(line)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c8b703-e312-4964-8be9-a754468e07cd",
   "metadata": {},
   "source": [
    "#### 清除模型Endpoint和config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f70d116f-4fb1-4f04-8732-3d6e4fb520de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws sagemaker delete-endpoint --endpoint-name $model_name\n",
    "!aws sagemaker delete-endpoint-config --endpoint-config-name $model_name\n",
    "!aws sagemaker delete-model --model-name $model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4f8df7b-16e0-4bb1-b5a4-7e7e6b308b25",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "duplicate argument 'top_p' in function definition (1471300743.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[37], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    def func(top_p,temperature,top_p,max_length):\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m duplicate argument 'top_p' in function definition\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "  \"max_length\": 1024,\n",
    "  \"temperature\": 0.5,\n",
    "  \"top_p\":0.9\n",
    "}\n",
    "def func(top_p,temperature,top_p,max_length):\n",
    "    return\n",
    "func(top=0.8,**parameters)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
