{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c261e5f4-17a8-40da-beb9-599f1717e0fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. 下载模型到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fba6e1f1-d9f6-4c9e-8e30-928dbe34b398",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded plugins: dkms-build-requires, extras_suggestions, kernel-livepatch,\n",
      "              : langpacks, priorities, update-motd, versionlock\n",
      "https://download.docker.com/linux/centos/2/x86_64/stable/repodata/repomd.xml: [Errno 14] HTTPS Error 404 - Not Found\n",
      "Trying other mirror.\n",
      "libnvidia-container/x86_64/signature                     |  833 B     00:00     \n",
      "libnvidia-container/x86_64/signature                     | 2.1 kB     00:02 !!! \n",
      "neuron                                                   | 2.9 kB     00:00     \n",
      "(1/2): libnvidia-container/x86_64/primary                  |  32 kB   00:00     \n",
      "(2/2): neuron/primary_db                                   | 153 kB   00:00     \n",
      "(1/8): amzn2extra-epel/2/x86_64/primary_db                 | 1.8 kB   00:00     \n",
      "(2/8): amzn2extra-livepatch/2/x86_64/primary_db            |  51 kB   00:00     \n",
      "(3/8): amzn2extra-docker/2/x86_64/primary_db               | 104 kB   00:00     \n",
      "(4/8): copr:copr.fedorainfracloud.org:vbatts:shadow-utils- | 6.1 kB   00:00     \n",
      "(5/8): epel/x86_64/primary_db                              | 7.0 MB   00:00     \n",
      "(6/8): amzn2extra-kernel-5.10/2/x86_64/primary_db          |  21 MB   00:00     \n",
      "(7/8): centos-extras/primary_db                            | 250 kB   00:00     \n",
      "(8/8): amzn2-core/2/x86_64/primary_db                      |  68 MB   00:01     \n",
      "amzn2extra-python3.8/2/x86_64/primary                    |  16 kB     00:00     \n",
      "libnvidia-container                                                     211/211\n",
      "288 packages excluded due to repository priority protections\n",
      "amzn2-core/2/x86_64/group_gz                             | 2.7 kB     00:00     \n",
      "epel/x86_64/group_gz                                     |  99 kB     00:00     \n",
      "No packages marked for update\n",
      "Loaded plugins: dkms-build-requires, extras_suggestions, kernel-livepatch,\n",
      "              : langpacks, priorities, update-motd, versionlock\n",
      "https://download.docker.com/linux/centos/2/x86_64/stable/repodata/repomd.xml: [Errno 14] HTTPS Error 404 - Not Found\n",
      "Trying other mirror.\n",
      "neuron                                                   | 2.9 kB     00:00     \n",
      "288 packages excluded due to repository priority protections\n",
      "Package amazon-linux-extras-2.0.3-1.amzn2.noarch already installed and latest version\n",
      "Nothing to do\n",
      "Installing epel-release\n",
      "Loaded plugins: dkms-build-requires, extras_suggestions, kernel-livepatch,\n",
      "              : langpacks, priorities, update-motd, versionlock\n",
      "Cleaning repos: amzn2-core amzn2extra-docker amzn2extra-epel\n",
      "              : amzn2extra-kernel-5.10 amzn2extra-livepatch amzn2extra-python3.8\n",
      "              : centos-extras\n",
      "              : copr:copr.fedorainfracloud.org:vbatts:shadow-utils-newxidmap\n",
      "              : docker-ce-stable epel libnvidia-container neuron\n",
      "39 metadata files removed\n",
      "20 sqlite files removed\n",
      "0 metadata files removed\n",
      "Loaded plugins: dkms-build-requires, extras_suggestions, kernel-livepatch,\n",
      "              : langpacks, priorities, update-motd, versionlock\n",
      "amzn2-core                                               | 3.6 kB     00:00     \n",
      "amzn2extra-docker                                        | 2.9 kB     00:00     \n",
      "amzn2extra-epel                                          | 3.0 kB     00:00     \n",
      "amzn2extra-kernel-5.10                                   | 3.0 kB     00:00     \n",
      "amzn2extra-livepatch                                     | 2.9 kB     00:00     \n",
      "amzn2extra-python3.8                                     | 2.9 kB     00:00     \n",
      "centos-extras                                            | 2.9 kB     00:00     \n",
      "copr:copr.fedorainfracloud.org:vbatts:shadow-utils-newxi | 3.3 kB     00:00     \n",
      "https://download.docker.com/linux/centos/2/x86_64/stable/repodata/repomd.xml: [Errno 14] HTTPS Error 404 - Not Found\n",
      "Trying other mirror.\n",
      "epel/x86_64/metalink                                     |  26 kB     00:00     \n",
      "epel                                                     | 4.7 kB     00:00     \n",
      "libnvidia-container/x86_64/signature                     |  833 B     00:00     \n",
      "libnvidia-container/x86_64/signature                     | 2.1 kB     00:02 !!! \n",
      "neuron                                                   | 2.9 kB     00:00     \n",
      "(1/20): amzn2-core/2/x86_64/group_gz                       | 2.7 kB   00:00     \n",
      "(2/20): amzn2-core/2/x86_64/updateinfo                     | 742 kB   00:00     \n",
      "(3/20): amzn2extra-epel/2/x86_64/primary_db                | 1.8 kB   00:00     \n",
      "(4/20): amzn2extra-docker/2/x86_64/primary_db              | 104 kB   00:00     \n",
      "(5/20): amzn2extra-epel/2/x86_64/updateinfo                |   76 B   00:00     \n",
      "(6/20): amzn2extra-docker/2/x86_64/updateinfo              |  13 kB   00:00     \n",
      "(7/20): amzn2extra-kernel-5.10/2/x86_64/updateinfo         |  42 kB   00:00     \n",
      "(8/20): amzn2extra-livepatch/2/x86_64/updateinfo           |  17 kB   00:00     \n",
      "(9/20): amzn2extra-livepatch/2/x86_64/primary_db           |  51 kB   00:00     \n",
      "(10/20): amzn2extra-python3.8/2/x86_64/updateinfo          | 4.3 kB   00:00     \n",
      "(11/20): amzn2extra-python3.8/2/x86_64/primary             |  16 kB   00:00     \n",
      "(12/20): copr:copr.fedorainfracloud.org:vbatts:shadow-util | 6.1 kB   00:00     \n",
      "(13/20): epel/x86_64/group_gz                              |  99 kB   00:00     \n",
      "(14/20): amzn2extra-kernel-5.10/2/x86_64/primary_db        |  21 MB   00:00     \n",
      "(15/20): epel/x86_64/updateinfo                            | 1.0 MB   00:00     \n",
      "(16/20): centos-extras/primary_db                          | 250 kB   00:00     \n",
      "(17/20): epel/x86_64/primary_db                            | 7.0 MB   00:00     \n",
      "(18/20): libnvidia-container/x86_64/primary                |  32 kB   00:00     \n",
      "(19/20): neuron/primary_db                                 | 153 kB   00:00     \n",
      "(20/20): amzn2-core/2/x86_64/primary_db                    |  68 MB   00:00     \n",
      "libnvidia-container                                                     211/211\n",
      "288 packages excluded due to repository priority protections\n",
      "Package epel-release-7-11.noarch already installed and latest version\n",
      "Nothing to do\n",
      "  2  httpd_modules            available  \u001b[0m  [ =1.0  =stable ]\n",
      "  3  memcached1.5             available  \u001b[0m  \\\n",
      "        [ =1.5.1  =1.5.16  =1.5.17 ]\n",
      "  9  R3.4                     available  \u001b[0m  [ =3.4.3  =stable ]\n",
      " 10  rust1                    available  \u001b[0m  \\\n",
      "        [ =1.22.1  =1.26.0  =1.26.1  =1.27.2  =1.31.0  =1.38.0\n",
      "          =stable ]\n",
      " 18  libreoffice              available  \u001b[0m  \\\n",
      "        [ =5.0.6.2_15  =5.3.6.1  =stable ]\n",
      " 19  gimp                     available  \u001b[0m  [ =2.8.22 ]\n",
      " 20 †\u001b[94mdocker=latest            enabled    \u001b[0m  \\\n",
      "        [ =17.12.1  =18.03.1  =18.06.1  =18.09.9  =stable ]\n",
      " 21  mate-desktop1.x          available  \u001b[0m  \\\n",
      "        [ =1.19.0  =1.20.0  =stable ]\n",
      " 22  GraphicsMagick1.3        available  \u001b[0m  \\\n",
      "        [ =1.3.29  =1.3.32  =1.3.34  =stable ]\n",
      " 23 †tomcat8.5                available  \u001b[0m  \\\n",
      "        [ =8.5.31  =8.5.32  =8.5.38  =8.5.40  =8.5.42  =8.5.50\n",
      "          =stable ]\n",
      " 24  \u001b[94mepel=latest              enabled    \u001b[0m  [ =7.11  =stable ]\n",
      " 25  testing                  available  \u001b[0m  [ =1.0  =stable ]\n",
      " 26  ecs                      available  \u001b[0m  [ =stable ]\n",
      " 27 †corretto8                available  \u001b[0m  \\\n",
      "        [ =1.8.0_192  =1.8.0_202  =1.8.0_212  =1.8.0_222  =1.8.0_232\n",
      "          =1.8.0_242  =stable ]\n",
      " 32  lustre2.10               available  \u001b[0m  \\\n",
      "        [ =2.10.5  =2.10.8  =stable ]\n",
      " 33 †java-openjdk11           available  \u001b[0m  [ =11  =stable ]\n",
      " 34  lynis                    available  \u001b[0m  [ =stable ]\n",
      " 36  BCC                      available  \u001b[0m  [ =0.x  =stable ]\n",
      " 37  mono                     available  \u001b[0m  [ =5.x  =stable ]\n",
      " 38  nginx1                   available  \u001b[0m  [ =stable ]\n",
      " 40  mock                     available  \u001b[0m  [ =stable ]\n",
      " 43  \u001b[94mlivepatch=latest         enabled    \u001b[0m  [ =stable ]\n",
      " 44 †\u001b[94mpython3.8=latest         enabled    \u001b[0m  [ =stable ]\n",
      " 45  haproxy2                 available  \u001b[0m  [ =stable ]\n",
      " 46  collectd                 available  \u001b[0m  [ =stable ]\n",
      " 47  aws-nitro-enclaves-cli   available  \u001b[0m  [ =stable ]\n",
      " 48  R4                       available  \u001b[0m  [ =stable ]\n",
      "  _  kernel-5.4               available  \u001b[0m  [ =stable ]\n",
      " 50  selinux-ng               available  \u001b[0m  [ =stable ]\n",
      " 52  tomcat9                  available  \u001b[0m  [ =stable ]\n",
      " 53  unbound1.13              available  \u001b[0m  [ =stable ]\n",
      " 54 †mariadb10.5              available  \u001b[0m  [ =stable ]\n",
      " 55  \u001b[94mkernel-5.10=latest       enabled    \u001b[0m  [ =stable ]\n",
      " 56  redis6                   available  \u001b[0m  [ =stable ]\n",
      " 57 †ruby3.0                  available  \u001b[0m  [ =stable ]\n",
      " 58 †postgresql12             available  \u001b[0m  [ =stable ]\n",
      " 59 †postgresql13             available  \u001b[0m  [ =stable ]\n",
      " 60  mock2                    available  \u001b[0m  [ =stable ]\n",
      " 61  dnsmasq2.85              available  \u001b[0m  [ =stable ]\n",
      " 62  kernel-5.15              available  \u001b[0m  [ =stable ]\n",
      " 63 †postgresql14             available  \u001b[0m  [ =stable ]\n",
      " 64  firefox                  available  \u001b[0m  [ =stable ]\n",
      " 65  lustre                   available  \u001b[0m  [ =stable ]\n",
      " 66 †php8.1                   available  \u001b[0m  [ =stable ]\n",
      " 67  awscli1                  available  \u001b[0m  [ =stable ]\n",
      " 68 †php8.2                   available  \u001b[0m  [ =stable ]\n",
      " 69  dnsmasq                  available  \u001b[0m  [ =stable ]\n",
      " 70  unbound1.17              available  \u001b[0m  [ =stable ]\n",
      " 72  collectd-python3         available  \u001b[0m  [ =stable ]\n",
      "† Note on end-of-support. Use 'info' subcommand.\n",
      "Loaded plugins: dkms-build-requires, extras_suggestions, kernel-livepatch,\n",
      "              : langpacks, priorities, update-motd, versionlock\n",
      "https://download.docker.com/linux/centos/2/x86_64/stable/repodata/repomd.xml: [Errno 14] HTTPS Error 404 - Not Found\n",
      "Trying other mirror.\n",
      "neuron                                                   | 2.9 kB     00:00     \n",
      "288 packages excluded due to repository priority protections\n",
      "No packages marked for update\n",
      "Loaded plugins: dkms-build-requires, extras_suggestions, kernel-livepatch,\n",
      "              : langpacks, priorities, update-motd, versionlock\n",
      "https://download.docker.com/linux/centos/2/x86_64/stable/repodata/repomd.xml: [Errno 14] HTTPS Error 404 - Not Found\n",
      "Trying other mirror.\n",
      "neuron                                                   | 2.9 kB     00:00     \n",
      "288 packages excluded due to repository priority protections\n",
      "Package git-lfs-2.10.0-2.el7.x86_64 already installed and latest version\n",
      "Package git-2.40.1-1.amzn2.0.1.x86_64 already installed and latest version\n",
      "Nothing to do\n"
     ]
    }
   ],
   "source": [
    "# For notebook instances (Amazon Linux)\n",
    "!sudo yum update -y\n",
    "!sudo yum install amazon-linux-extras\n",
    "!sudo amazon-linux-extras install epel -y\n",
    "!sudo yum update -y\n",
    "!sudo yum install git-lfs git -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1df1c7e4-d22a-4088-ab70-418d751d0091",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.wisemodel.cn/Baichuan-inc/Baichuan2-13B-Chat-4bits.git\n",
      "Error: failed to call git rev-parse --git-dir: exit status 128 : fatal: detected dubious ownership in repository at '/home/ec2-user/SageMaker/guidance-for-custom-search-of-an-enterprise-knowledge-base-on-aws'\n",
      "To add an exception for this directory, call:\n",
      "\n",
      "\tgit config --global --add safe.directory /home/ec2-user/SageMaker/guidance-for-custom-search-of-an-enterprise-knowledge-base-on-aws\n",
      "\n",
      "Git LFS initialized.\n",
      "Cloning into 'Baichuan2-13B-Chat-4bits'...\n",
      "remote: Enumerating objects: 21, done.\u001b[K\n",
      "remote: Total 21 (delta 0), reused 0 (delta 0), pack-reused 21\u001b[K\n",
      "Receiving objects: 100% (21/21), 442.80 KiB | 431.00 KiB/s, done.\n",
      "Resolving deltas: 100% (1/1), done.\n",
      "Filtering content: 100% (2/2), 469.38 MiB | 387.00 KiB/s, done.\n",
      "Encountered 1 file(s) that may not have been copied correctly on Windows:\n",
      "\tpytorch_model.bin\n",
      "\n",
      "See: `git lfs help smudge` for more details.\n"
     ]
    }
   ],
   "source": [
    "#下载模型snapshot到本地，需要25G空间\n",
    "#需大约15-30分钟时间，请耐心等待, 如果左侧大括号内还是[*]，就还在下载中，*变成任意数例如[3]就证明已完成\n",
    "\n",
    "from pathlib import Path\n",
    "local_model_path = Path(\"./Baichuan2-13B-Chat-4bits\")\n",
    "local_model_path.mkdir(exist_ok=True)\n",
    "model_name = \"Baichuan-inc/Baichuan2-13B-Chat-4bits\"\n",
    "clone_path = f\"https://www.wisemodel.cn/{model_name}.git\"\n",
    "print(clone_path)\n",
    "\n",
    "!git lfs install\n",
    "!git clone $clone_path\n",
    "!cd ./Baichuan2-13B-Chat-4bits && rm -rf .git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e889dcf-a093-497d-902f-f6f130b69c01",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. 把模型拷贝到S3为后续部署做准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ba78615-76a3-46ad-a757-bf8f71a802dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "model_snapshot_path: Baichuan2-13B-Chat-4bits\n",
      "s3_location: s3://sagemaker-us-east-1-340636688520/llm_model/Baichuan2-13B-Chat-4bits/\n",
      "s3_code_prefix: lmi_inference_code/Baichuan2-13B-Chat-4bits\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "sagemaker_session_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "region = sagemaker_session._region_name\n",
    "account_id = sagemaker_session.account_id()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "s3_code_prefix = f\"lmi_inference_code/{model_name.split('/')[-1]}\"\n",
    "\n",
    "s3_location = f\"s3://{sagemaker_session_bucket}/llm_model/{model_name.split('/')[-1]}/\"\n",
    "\n",
    "#你也可以把local_model_path直接替换成你的模型路径，例\"model_snapshot_path=./chatglm3-6b\", 这个文件夹里需要包含config.json\n",
    "model_snapshot_path = local_model_path\n",
    "\n",
    "print(f\"model_snapshot_path: {model_snapshot_path}\")\n",
    "print(\"s3_location:\",s3_location)\n",
    "print(\"s3_code_prefix:\",s3_code_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62f2fc81-329d-4f41-84a7-15f056a1e4a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: Baichuan2-13B-Chat-4bits/Baichuan2 模型社区许可协议.pdf to s3://sagemaker-us-east-1-340636688520/llm_model/Baichuan2-13B-Chat-4bits/Baichuan2 模型社区许可协议.pdf\n",
      "upload: Baichuan2-13B-Chat-4bits/configuration_baichuan.py to s3://sagemaker-us-east-1-340636688520/llm_model/Baichuan2-13B-Chat-4bits/configuration_baichuan.py\n",
      "upload: Baichuan2-13B-Chat-4bits/generation_config.json to s3://sagemaker-us-east-1-340636688520/llm_model/Baichuan2-13B-Chat-4bits/generation_config.json\n",
      "upload: Baichuan2-13B-Chat-4bits/.gitattributes to s3://sagemaker-us-east-1-340636688520/llm_model/Baichuan2-13B-Chat-4bits/.gitattributes\n",
      "upload: Baichuan2-13B-Chat-4bits/generation_utils.py to s3://sagemaker-us-east-1-340636688520/llm_model/Baichuan2-13B-Chat-4bits/generation_utils.py\n",
      "upload: Baichuan2-13B-Chat-4bits/config.json to s3://sagemaker-us-east-1-340636688520/llm_model/Baichuan2-13B-Chat-4bits/config.json\n",
      "upload: Baichuan2-13B-Chat-4bits/handler.py to s3://sagemaker-us-east-1-340636688520/llm_model/Baichuan2-13B-Chat-4bits/handler.py\n",
      "upload: Baichuan2-13B-Chat-4bits/README.md to s3://sagemaker-us-east-1-340636688520/llm_model/Baichuan2-13B-Chat-4bits/README.md\n",
      "upload: Baichuan2-13B-Chat-4bits/modeling_baichuan.py to s3://sagemaker-us-east-1-340636688520/llm_model/Baichuan2-13B-Chat-4bits/modeling_baichuan.py\n",
      "upload: Baichuan2-13B-Chat-4bits/Community License for Baichuan2 Model.pdf to s3://sagemaker-us-east-1-340636688520/llm_model/Baichuan2-13B-Chat-4bits/Community License for Baichuan2 Model.pdf\n",
      "upload: Baichuan2-13B-Chat-4bits/requirements.txt to s3://sagemaker-us-east-1-340636688520/llm_model/Baichuan2-13B-Chat-4bits/requirements.txt\n",
      "upload: Baichuan2-13B-Chat-4bits/quantizer.py to s3://sagemaker-us-east-1-340636688520/llm_model/Baichuan2-13B-Chat-4bits/quantizer.py\n",
      "upload: Baichuan2-13B-Chat-4bits/special_tokens_map.json to s3://sagemaker-us-east-1-340636688520/llm_model/Baichuan2-13B-Chat-4bits/special_tokens_map.json\n",
      "upload: Baichuan2-13B-Chat-4bits/tokenization_baichuan.py to s3://sagemaker-us-east-1-340636688520/llm_model/Baichuan2-13B-Chat-4bits/tokenization_baichuan.py\n",
      "upload: Baichuan2-13B-Chat-4bits/tokenizer.model to s3://sagemaker-us-east-1-340636688520/llm_model/Baichuan2-13B-Chat-4bits/tokenizer.model\n",
      "upload: Baichuan2-13B-Chat-4bits/tokenizer_config.json to s3://sagemaker-us-east-1-340636688520/llm_model/Baichuan2-13B-Chat-4bits/tokenizer_config.json\n",
      "upload: Baichuan2-13B-Chat-4bits/pytorch_model.bin to s3://sagemaker-us-east-1-340636688520/llm_model/Baichuan2-13B-Chat-4bits/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "#上传模型\n",
    "!aws s3 sync $model_snapshot_path $s3_location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696b70c3-90f1-4175-95bf-568bafbcd383",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. 模型部署准备（entrypoint脚本，容器镜像，服务配置）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6f7c4277-4480-42c6-aee6-1fbcca94eb82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image going to be used is ---- > 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.24.0-deepspeed0.10.0-cu118\n"
     ]
    }
   ],
   "source": [
    "inference_image_uri = (\n",
    "    f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.24.0-deepspeed0.10.0-cu118\"\n",
    ")\n",
    "if \"cn-\" in region:\n",
    "    inference_image_uri = (\n",
    "        f\"727897471807.dkr.ecr.{region}.amazonaws.com.cn/djl-inference:0.24.0-deepspeed0.10.0-cu118\"\n",
    "    )\n",
    "\n",
    "\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8d771bdb-11d2-45d2-9bef-face29221838",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p LLM_baichuan_deploy_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c9a3df19-fc7d-4a68-aa90-9f278262b618",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting LLM_baichuan_deploy_code/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile LLM_baichuan_deploy_code/model.py\n",
    "from djl_python import Input, Output\n",
    "import torch\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "from transformers import pipeline, AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "import deepspeed\n",
    "\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "\n",
    "\n",
    "def load_model(properties):\n",
    "    tensor_parallel_degree = properties[\"tensor_parallel_degree\"]\n",
    "    model_location = properties['model_dir']\n",
    "    if \"model_id\" in properties:\n",
    "        model_location = properties['model_id']\n",
    "    logging.info(f\"Loading model in {model_location}\")\n",
    "    \n",
    "    \n",
    "    print('============================tokenizer ..====================')\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_location, trust_remote_code=True)\n",
    "    \n",
    "    print('============================model====================')\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(model_location, device_map=\"auto\",\n",
    "                                                 trust_remote_code=True)\n",
    "    \n",
    "    model.generation_config = GenerationConfig.from_pretrained(model_location)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "generator = None\n",
    "\n",
    "##zpf\n",
    "def construct_message(history,prompt):\n",
    "    message = []\n",
    "    for question, answer in history:\n",
    "        message.append({\"role\":\"user\",\"content\":question})\n",
    "        message.append({\"role\":\"assistant\",\"content\":answer})\n",
    "    message.append({\"role\":\"user\",\"content\":prompt})\n",
    "    return message\n",
    "def stream_items(prompt, history, max_length, top_p, temperature):\n",
    "    global model, tokenizer\n",
    "    size = 0\n",
    "    response = \"\"\n",
    "    model.generation_config.max_new_tokens = max_length\n",
    "    model.generation_config.top_p = top_p\n",
    "    \n",
    "    messages = construct_message(history,prompt)\n",
    "    res_generator = model.chat(tokenizer, messages,stream=True)\n",
    "    for response in res_generator:\n",
    "        this_response = response[size:]\n",
    "        size = len(response)\n",
    "        stream_buffer = { \"outputs\":this_response,\"finished\": False}\n",
    "        yield stream_buffer\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    global model, tokenizer\n",
    "    if not model:\n",
    "        model, tokenizer = load_model(inputs.get_properties())\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        return None\n",
    "    data = inputs.get_as_json()\n",
    "    \n",
    "    logging.info(\"starting debug\")\n",
    "    logging.info(f\"inputs:{data}\")\n",
    "    \n",
    "    \n",
    "    #zpf\n",
    "    input_sentences = data[\"ask\"]\n",
    "    params={}\n",
    "    if \"parameters\" in data:\n",
    "        params = data[\"parameters\"]\n",
    "    else:\n",
    "        params = {\"temperature\": data[\"temperature\"]}\n",
    "    history=[]\n",
    "    if \"history\" in data:\n",
    "        history = data[\"history\"]\n",
    "    stream=False\n",
    "    if \"stream\" in data:\n",
    "        stream = data.get('stream')\n",
    "    print(f'input prompt:{input_sentences}')  \n",
    "    outputs = Output()\n",
    "    if stream:\n",
    "        outputs.add_property(\"content-type\", \"application/jsonlines\")\n",
    "        outputs.add_stream_content(stream_items(input_sentences,history=history,**params))\n",
    "    else:\n",
    "        messages = []\n",
    "        messages.append({\"role\": \"user\", \"content\": input_sentences})\n",
    "        response = model.chat(tokenizer, messages)\n",
    "        result = {\"answer\": response}\n",
    "        outputs.add_as_json(result)\n",
    "    \n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8996fe44-8e70-468b-abc1-38187cb33f4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting LLM_baichuan_deploy_code/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile LLM_baichuan_deploy_code/serving.properties\n",
    "engine=Python\n",
    "option.tensor_parallel_degree=1\n",
    "option.model_id=s3://sagemaker-us-east-1-340636688520/llm_model/Baichuan2-13B-Chat-4bits/\n",
    "option.enable_streaming=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f906d376-0ec3-4dcf-9109-8647285b80eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#将模型的s3路径更新到inference.py中\n",
    "!sed -i 's|option.model_id=.*|option.model_id={s3_location}|' LLM_baichuan_deploy_code/serving.properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7b7e76c6-6dbc-47fc-9f47-4765c526ab76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting LLM_baichuan_deploy_code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile LLM_baichuan_deploy_code/requirements.txt\n",
    "-i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "transformers==4.33.1\n",
    "accelerate>=0.17.1\n",
    "einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0ae6734a-aacd-410d-818d-0a962697c3c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM_baichuan_deploy_code/\n",
      "LLM_baichuan_deploy_code/requirements.txt\n",
      "LLM_baichuan_deploy_code/model.py\n",
      "LLM_baichuan_deploy_code/serving.properties\n"
     ]
    }
   ],
   "source": [
    "!rm model.tar.gz\n",
    "!cd LLM_baichuan_deploy_code && rm -rf \".ipynb_checkpoints\"\n",
    "!tar czvf model.tar.gz LLM_baichuan_deploy_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0f77dc76-6d8c-4665-ba88-f03e887c136c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-east-1-340636688520/lmi_inference_code/Baichuan2-13B-Chat-4bits/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_code_artifact = sagemaker_session.upload_data(\"model.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5853daa-b8a3-4485-8c0a-64bf83e93a18",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. 创建模型 & 创建endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ef974ca1-9638-45a8-9145-ea9d03b2b072",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch-inference-llm-v1\n",
      "Image going to be used is ---- > 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.24.0-deepspeed0.10.0-cu118\n",
      "Created Model: arn:aws:sagemaker:us-east-1:340636688520:model/pytorch-inference-llm-v1\n",
      "{'EndpointConfigArn': 'arn:aws:sagemaker:us-east-1:340636688520:endpoint-config/pytorch-inference-llm-v1', 'ResponseMetadata': {'RequestId': 'd035d571-5fa4-44da-adc9-ada282e762d6', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'd035d571-5fa4-44da-adc9-ada282e762d6', 'content-type': 'application/x-amz-json-1.1', 'content-length': '105', 'date': 'Sun, 17 Dec 2023 12:47:15 GMT'}, 'RetryAttempts': 0}}\n",
      "Created Endpoint: arn:aws:sagemaker:us-east-1:340636688520:endpoint/pytorch-inference-llm-v1\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "import boto3\n",
    "\n",
    "model_name = 'pytorch-inference-llm-v1'\n",
    "print(model_name)\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": inference_image_uri,\n",
    "        \"ModelDataUrl\": s3_code_artifact\n",
    "    },\n",
    "    \n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")\n",
    "\n",
    "endpoint_config_name = model_name\n",
    "endpoint_name = model_name\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.g4dn.4xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            # \"VolumeSizeInGB\" : 400,\n",
    "            # \"ModelDataDownloadTimeoutInSeconds\": 2400,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 15*60,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(endpoint_config_response)\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1262e826-a810-401d-a5a9-f62febb24e5f",
   "metadata": {},
   "source": [
    "#### 持续检测模型部署进度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08969928-6b9e-4d9c-a033-a31f5f77bdfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d173b9d8-3b8f-434b-b3a6-810af3693b25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "\n",
    "class LineIterator:\n",
    "    \"\"\"\n",
    "    A helper class for parsing the InvokeEndpointWithResponseStream event stream. \n",
    "    \n",
    "    The output of the model will be in the following format:\n",
    "    ```\n",
    "    b'{\"outputs\": [\" a\"]}\\n'\n",
    "    b'{\"outputs\": [\" challenging\"]}\\n'\n",
    "    b'{\"outputs\": [\" problem\"]}\\n'\n",
    "    ...\n",
    "    ```\n",
    "    \n",
    "    While usually each PayloadPart event from the event stream will contain a byte array \n",
    "    with a full json, this is not guaranteed and some of the json objects may be split across\n",
    "    PayloadPart events. For example:\n",
    "    ```\n",
    "    {'PayloadPart': {'Bytes': b'{\"outputs\": '}}\n",
    "    {'PayloadPart': {'Bytes': b'[\" problem\"]}\\n'}}\n",
    "    ```\n",
    "    \n",
    "    This class accounts for this by concatenating bytes written via the 'write' function\n",
    "    and then exposing a method which will return lines (ending with a '\\n' character) within\n",
    "    the buffer via the 'readlines' function. It maintains the position of the last read \n",
    "    position to ensure that previous bytes are not exposed again. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.buff = io.BytesIO()\n",
    "        self.read_pos = 0\n",
    "        \n",
    "    def write(self, content):\n",
    "        self.buff.seek(0, io.SEEK_END)\n",
    "        self.buff.write(content)\n",
    "        \n",
    "    def readlines(self):\n",
    "        self.buff.seek(self.read_pos)\n",
    "        for line in self.buff.readlines():\n",
    "            if line[-1] != b'\\n':\n",
    "                self.read_pos += len(line)\n",
    "                yield line[:-1]\n",
    "                \n",
    "    def reset(self):\n",
    "        self.read_pos = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "aed1231b-7cb1-4bcb-b9c7-cdf339a5baf0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Life is a complex and multifaceted phenomenon that encompasses all aspects of existence, including thought, emotion, consciousness, and purpose. It is characterized by growth, development, and change, and it involves interacting with the environment and other living things.\n",
      "\n",
      "In its most basic form, life is a property possessed by certain matter, usually organic matter, which is capable of maintaining homeostasis, reproducing, and adapting to changes. This definition is often attributed to British biologist Alan Turing, who proposed the Turing test as a way to determine if something has the characteristics of a living being.\n",
      "\n",
      "Life on Earth includes a vast array of organisms, from single-celled bacteria to multi-celled plants and animals. These living beings can be classified into different kingdoms, such as animals, plants, fungi, and protists, based on their shared and genetic makeup.\n",
      "\n",
      "The study of life and its various forms is known as biology, which aims to understand the mechanisms that underpin the functioning and evolution of living organisms. This field of research has led to numerous discoveries about the molecular basis of life, such as DNA, RNA, proteins, and other biomolecules, as well as the processes that drive cellular function and reproduction.\n",
      "\n",
      "In addition to its biological aspects, life also encompasses the human experience, including culture, society, and the pursuit of happiness, fulfillment, and meaning. Philosophers,, and social scientists have studied the nature of human experience and sought to understand what it means to live a good life."
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "  \"max_length\": 1024,\n",
    "  \"temperature\": 0.5,\n",
    "  \"top_p\":0.9\n",
    "}\n",
    "body = {\"ask\": \"what is life\", \"parameters\": parameters,\"history\" : [],\"stream\":True}\n",
    "resp = smr_client.invoke_endpoint_with_response_stream(EndpointName=endpoint_name, Body=json.dumps(body), ContentType=\"application/json\")\n",
    "event_stream = resp['Body']\n",
    "\n",
    "scanner = LineIterator()\n",
    "for event in event_stream:\n",
    "    scanner.write(event['PayloadPart']['Bytes'])\n",
    "    for line in scanner.readlines():\n",
    "        try:\n",
    "            resp = json.loads(line)\n",
    "            # print(resp)\n",
    "            print(resp.get(\"outputs\")['outputs'], end='')\n",
    "        except Exception as e:\n",
    "            # print(line)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5a62b04e-db2a-41d7-a22f-ffca881d9a6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (424) from primary with message \"{\n  \"code\":424,\n  \"message\":\"prediction failure\",\n  \"error\":\"name 'stream' is not defined\"\n}\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/pytorch-inference-llm-v1 in account 340636688520 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 90\u001b[0m\n\u001b[1;32m     82\u001b[0m prompts3\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m好累啊\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_sample\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.9\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepetition_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1.03\u001b[39m\n\u001b[1;32m     89\u001b[0m }\n\u001b[0;32m---> 90\u001b[0m response_model \u001b[38;5;241m=\u001b[39m \u001b[43msmr_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_endpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m            \u001b[49m\u001b[43mEndpointName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m            \u001b[49m\u001b[43mBody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts3\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m            \u001b[49m\u001b[43mContentType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapplication/json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m response_model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBody\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/client.py:553\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    550\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    551\u001b[0m     )\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/client.py:1009\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1006\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1007\u001b[0m     )\n\u001b[1;32m   1008\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (424) from primary with message \"{\n  \"code\":424,\n  \"message\":\"prediction failure\",\n  \"error\":\"name 'stream' is not defined\"\n}\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/pytorch-inference-llm-v1 in account 340636688520 for more information."
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "endpoint_name = \"pytorch-inference-llm-v1\"\n",
    "prompts1 = \"\"\"\n",
    "你是MySQL的专家。给定一个输入问题，创建一个语法正确的MySQL查询语句。\n",
    "除非用户在问题中指定了要获得的特定数量的示例，否则使用LIMIT子句查询最多3个结果。您可以对结果进行排序，以返回数据库中信息量最大的数据。您必须仅查询回答问题所需的列。将每个列名用反引号（`）括起来，表示为分隔的标识符。\n",
    "请注意，仅可以使用在下面这些表中看到的列名，不要查询不存在的列。此外，还要注意哪个列在哪个表中。如果问题涉及”今天”，请注意使用CURDATE()函数获取当前日期.\n",
    "\n",
    "使用如下格式:\n",
    "Question: 具体的问题\n",
    "SQLQuery: 运行的sql语句\n",
    "SQLResult: SQLQuery运行的结果\n",
    "Answer: 最终的回答\n",
    "\n",
    "\n",
    "使用如下的表:\n",
    "CREATE TABLE customer (\n",
    "\tc_customer_sk INTEGER NOT NULL, \n",
    "\tc_customer_id CHAR(16) NOT NULL, \n",
    "\tc_current_cdemo_sk INTEGER, \n",
    "\tc_current_hdemo_sk INTEGER, \n",
    "\tc_current_addr_sk INTEGER, \n",
    "\tc_first_shipto_date_sk INTEGER, \n",
    "\tc_first_sales_date_sk INTEGER, \n",
    "\tc_salutation CHAR(10), \n",
    "\tc_first_name CHAR(20), \n",
    "\tc_last_name CHAR(30), \n",
    "\tc_preferred_cust_flag CHAR(1), \n",
    "\tc_birth_day INTEGER, \n",
    "\tc_birth_month INTEGER, \n",
    "\tc_birth_year INTEGER, \n",
    "\tc_birth_country VARCHAR(20), \n",
    "\tc_login CHAR(13), \n",
    "\tc_email_address CHAR(50), \n",
    "\tc_last_review_date CHAR(10), \n",
    "\tPRIMARY KEY (c_customer_sk)\n",
    ")ENGINE=InnoDB DEFAULT CHARSET=utf8\n",
    "\n",
    "\n",
    "CREATE TABLE web_sales (\n",
    "\tws_sold_date_sk INTEGER, \n",
    "\tws_sold_time_sk INTEGER, \n",
    "\tws_ship_date_sk INTEGER, \n",
    "\tws_item_sk INTEGER NOT NULL, \n",
    "\tws_bill_customer_sk INTEGER, \n",
    "\tws_bill_cdemo_sk INTEGER, \n",
    "\tws_bill_hdemo_sk INTEGER, \n",
    "\tws_bill_addr_sk INTEGER, \n",
    "\tws_ship_customer_sk INTEGER, \n",
    "\tws_ship_cdemo_sk INTEGER, \n",
    "\tws_ship_hdemo_sk INTEGER, \n",
    "\tws_ship_addr_sk INTEGER, \n",
    "\tws_web_page_sk INTEGER, \n",
    "\tws_web_site_sk INTEGER, \n",
    "\tws_ship_mode_sk INTEGER, \n",
    "\tws_warehouse_sk INTEGER, \n",
    "\tws_promo_sk INTEGER, \n",
    "\tws_order_number INTEGER NOT NULL, \n",
    "\tws_quantity INTEGER, \n",
    "\tws_wholesale_cost DECIMAL(7, 2), \n",
    "\tws_list_price DECIMAL(7, 2), \n",
    "\tws_sales_price DECIMAL(7, 2), \n",
    "\tws_ext_discount_amt DECIMAL(7, 2), \n",
    "\tws_ext_sales_price DECIMAL(7, 2), \n",
    "\tws_ext_wholesale_cost DECIMAL(7, 2), \n",
    "\tws_ext_list_price DECIMAL(7, 2), \n",
    "\tws_ext_tax DECIMAL(7, 2), \n",
    "\tws_coupon_amt DECIMAL(7, 2), \n",
    "\tws_ext_ship_cost DECIMAL(7, 2), \n",
    "\tws_net_paid DECIMAL(7, 2), \n",
    "\tws_net_paid_inc_tax DECIMAL(7, 2), \n",
    "\tws_net_paid_inc_ship DECIMAL(7, 2), \n",
    "\tws_net_paid_inc_ship_tax DECIMAL(7, 2), \n",
    "\tws_net_profit DECIMAL(7, 2), \n",
    "\tPRIMARY KEY (ws_item_sk, ws_order_number)\n",
    ")ENGINE=InnoDB DEFAULT CHARSET=utf8\n",
    "\n",
    "Question: 我需要知道销售报表中，下单金额最大的客户email地址\n",
    "\"\"\"\n",
    "\n",
    "prompts2=\"给我一个青海和甘肃旅游的路线，8天7晚\"\n",
    "prompts3=\"好累啊\"\n",
    "parameters={\n",
    "    \"do_sample\": False,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 1,\n",
    "    \"max_new_tokens\": 300,\n",
    "    \"repetition_penalty\": 1.03\n",
    "}\n",
    "response_model = smr_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            Body=json.dumps(\n",
    "            {\n",
    "                \"ask\": prompts3,\n",
    "                \"parameters\": parameters,\n",
    "            }\n",
    "            ),\n",
    "            ContentType=\"application/json\",\n",
    "        )\n",
    "\n",
    "response_model['Body'].read().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c8b703-e312-4964-8be9-a754468e07cd",
   "metadata": {},
   "source": [
    "#### 清除模型Endpoint和config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f70d116f-4fb1-4f04-8732-3d6e4fb520de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws sagemaker delete-endpoint --endpoint-name pytorch-inference-llm-v1\n",
    "!aws sagemaker delete-endpoint-config --endpoint-config-name pytorch-inference-llm-v1\n",
    "!aws sagemaker delete-model --model-name pytorch-inference-llm-v1"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
