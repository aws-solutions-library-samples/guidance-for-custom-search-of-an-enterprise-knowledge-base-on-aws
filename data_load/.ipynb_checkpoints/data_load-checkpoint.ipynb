{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e116b23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade numpy --target ./python\n",
    "!pip install --upgrade numexpr --target ./python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94688a8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"./python\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import traceback\n",
    "import urllib.parse\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "import time\n",
    "from python.smart_search_dataload import SmartSearchDataload\n",
    "\n",
    "port = 443\n",
    "bulk_size = 10000000\n",
    "\n",
    "sm_client = boto3.client('secretsmanager')\n",
    "master_user = sm_client.get_secret_value(SecretId='opensearch-host-url')['SecretString']\n",
    "data= json.loads(master_user)\n",
    "es_host_name = data.get('host')\n",
    "host = es_host_name+'/' if es_host_name[-1] != '/' else es_host_name# cluster endpoint, for example: my-test-domain.us-east-1.es.amazonaws.com/\n",
    "host = host[8:-1]\n",
    "region = boto3.Session().region_name # e.g. cn-north-1\n",
    "print('host:',host)\n",
    "print('region:',region)\n",
    "\n",
    "# retrieve secret manager value by key using boto3                                             \n",
    "sm_client = boto3.client('secretsmanager')\n",
    "master_user = sm_client.get_secret_value(SecretId='opensearch-master-user')['SecretString']\n",
    "data= json.loads(master_user)\n",
    "username = data.get('username')\n",
    "password = data.get('password')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1833d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#导入文件在AOS的index名称\n",
    "index =  \"data_index_test_2\"   \n",
    "\n",
    "#导入文件的主要语种，目前支持 chinese 和 english 共2种\n",
    "language = \"chinese\"\n",
    "\n",
    "#部署在SageMaker的向量模型的endpoint名称，当值为：bedrock-titan-embed 时，使用bedrock的amazon.titan-embed-text-v1模型, 当值为bedrock-cohere-embed 时，\n",
    "#使用cohere.embed-multilingual-v3模型\n",
    "embedding_endpoint_name = \"huggingface-inference-eb\"\n",
    "\n",
    "#需要导入文件的存储路径\n",
    "local_path = \"../docs/\"\n",
    "\n",
    "#目前主要在英语文档拆分时用到，拆分后文档的最大token数量\n",
    "chunk_size = 1500\n",
    "\n",
    "#目前主要在英语文档拆分时用到，拆分后相邻文档的头尾最大重合token数量\n",
    "chunk_overlap = 10 \n",
    "\n",
    "#目前主要在拆分csv文档时用到，一行的内容如果token数超过该参数，就会对一行内容进行拆分\n",
    "sep_word_len = 2000  \n",
    "\n",
    "#目前主要在拆分QA格式的csv文档时用到，如果指定了title_name，在拆分时会确保单独对title列的文本进行向量化，在split_to_sentence_paragraph=True时使用\n",
    "qa_title_name = ''     \n",
    "\n",
    "#文档是否拆分为sentence、paragraph的格式，使用sentence文本进行向量化，使用多条sentence组合为paragraph，给到大模型推理\n",
    "split_to_sentence_paragraph = True    \n",
    "\n",
    "#使用多少条sentence组装为paragraph\n",
    "paragraph_include_sentence_num = 3  \n",
    "\n",
    "#需要向量化文本的最大字数，使用SageMaker部署的向量模型时使用\n",
    "text_max_length = 350   \n",
    "\n",
    "#PDF格式的文件使用，设置为true时会先将PDF文件转换为HTML文件进行逻辑段落的拆分,在split_to_sentence_paragraph=True时使用\n",
    "pdf_to_html = False    \n",
    "\n",
    "#写入AOS的文本字段名称，langchain默认为text\n",
    "text_field = 'paragraph' \n",
    "\n",
    "#写入AOS的向量字段名称，langchain默认为vector_field\n",
    "vector_field = 'sentence_vector'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e5bd59-49c9-4255-b37f-f16d2d9d151d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append(r\"./python\")\n",
    "\n",
    "search_qa = SmartSearchDataload()\n",
    "search_qa.init_cfg(index,\n",
    "                 username,\n",
    "                 password,\n",
    "                 host,\n",
    "                 port,\n",
    "                 region,\n",
    "                 embedding_endpoint_name,\n",
    "                 language=language\n",
    "                 )\n",
    "files = os.listdir(local_path)\n",
    "i = 0\n",
    "for file in files:\n",
    "    local_file = local_path+file\n",
    "    now1 = datetime.now()#begin time\n",
    "    loaded_files = search_qa.init_knowledge_vector(local_file,\n",
    "                                                   chunk_size,\n",
    "                                                   chunk_overlap,\n",
    "                                                   sep_word_len,\n",
    "                                                   qa_title_name,\n",
    "                                                   split_to_sentence_paragraph,\n",
    "                                                   paragraph_include_sentence_num,\n",
    "                                                   text_max_length,\n",
    "                                                   pdf_to_html,\n",
    "                                                   text_field,\n",
    "                                                   vector_field)\n",
    "    now2 = datetime.now()#endtime\n",
    "    i += 1\n",
    "    print(i,\" Complete the import of the following documents:\", str(loaded_files),\" File import takes time:\",now2-now1 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4359c7-ae3e-451c-b59e-8f798b2d954a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
