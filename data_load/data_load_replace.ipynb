{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e116b23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "Successfully installed numpy-1.26.4\n",
      "Collecting numexpr\n",
      "  Downloading numexpr-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
      "Collecting numpy>=1.13.3 (from numexpr)\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Downloading numexpr-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (375 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.2/375.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Installing collected packages: numpy, numexpr\n",
      "Successfully installed numexpr-2.9.0 numpy-1.26.4\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade numpy --target ./python\n",
    "!pip install --upgrade numexpr --target ./python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "94688a8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "host: search-smartsearch-vzgiatoudsiwoq6fqy4wgnprui.us-west-2.es.amazonaws.com\n",
      "region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"./python\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import traceback\n",
    "import urllib.parse\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "import time\n",
    "from python.smart_search_dataload import SmartSearchDataload\n",
    "\n",
    "port = 443\n",
    "bulk_size = 10000000\n",
    "\n",
    "sm_client = boto3.client('secretsmanager')\n",
    "master_user = sm_client.get_secret_value(SecretId='opensearch-host-url')['SecretString']\n",
    "data= json.loads(master_user)\n",
    "es_host_name = data.get('host')\n",
    "host = es_host_name+'/' if es_host_name[-1] != '/' else es_host_name# cluster endpoint, for example: my-test-domain.us-east-1.es.amazonaws.com/\n",
    "host = host[8:-1]\n",
    "region = boto3.Session().region_name # e.g. cn-north-1\n",
    "print('host:',host)\n",
    "print('region:',region)\n",
    "\n",
    "# retrieve secret manager value by key using boto3                                             \n",
    "sm_client = boto3.client('secretsmanager')\n",
    "master_user = sm_client.get_secret_value(SecretId='opensearch-master-user')['SecretString']\n",
    "data= json.loads(master_user)\n",
    "username = data.get('username')\n",
    "password = data.get('password')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c1833d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#导入文件在AOS的index名称\n",
    "index =  \"data_index_test\"   \n",
    "\n",
    "#导入文件的主要语种，目前支持 chinese 和 english 共2种\n",
    "language = \"chinese\"\n",
    "\n",
    "#部署在SageMaker的向量模型的endpoint名称，当值为：bedrock-titan-embed 时，使用bedrock的amazon.titan-embed-text-v1模型\n",
    "embedding_endpoint_name = \"bedrock-titan-embed\" #\"huggingface-inference-eb\"\n",
    "\n",
    "#需要导入文件的存储路径\n",
    "local_path = \"../docs/\"\n",
    "\n",
    "#目前主要在英语文档拆分时用到，拆分后文档的最大token数量\n",
    "chunk_size = 1500\n",
    "\n",
    "#目前主要在英语文档拆分时用到，拆分后相邻文档的头尾最大重合token数量\n",
    "chunk_overlap = 10 \n",
    "\n",
    "#目前主要在拆分csv文档时用到，一行的内容如果token数超过该参数，就会对一行内容进行拆分\n",
    "sep_word_len = 2000  \n",
    "\n",
    "#目前主要在拆分QA格式的csv文档时用到，如果指定了title_name，在拆分时会确保单独对title列的文本进行向量化，在split_to_sentence_paragraph=True时使用\n",
    "qa_title_name = ''     \n",
    "\n",
    "#文档是否拆分为sentence、paragraph的格式，使用sentence文本进行向量化，使用多条sentence组合为paragraph，给到大模型推理\n",
    "split_to_sentence_paragraph = True    \n",
    "\n",
    "#使用多少条sentence组装为paragraph\n",
    "paragraph_include_sentence_num = 3  \n",
    "\n",
    "#需要向量化文本的最大字数，使用SageMaker部署的向量模型时使用\n",
    "text_max_length = 350   \n",
    "\n",
    "#PDF格式的文件使用，设置为true时会先将PDF文件转换为HTML文件进行逻辑段落的拆分,在split_to_sentence_paragraph=True时使用\n",
    "pdf_to_html = False    \n",
    "\n",
    "#写入AOS的文本字段名称，langchain默认为text\n",
    "text_field = 'paragraph' \n",
    "\n",
    "#写入AOS的向量字段名称，langchain默认为vector_field\n",
    "vector_field = 'sentence_vector'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "67cab421-0ef2-4a30-a97e-59534ae88d12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def deleteOriginal(client,index_name,filename):\n",
    "    print(\"begin to delete original\")\n",
    "    query = {\n",
    "      \"query\": {\n",
    "        \"wildcard\": {\n",
    "          \"metadata.source.keyword\": {\n",
    "            \"value\": f\"*{filename}\"\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    #print(f\"{query}:{index_name}\")\n",
    "    # 使用delete_by_query API删除匹配的文档\n",
    "    response = client.delete_by_query(index=index_name, body=query)\n",
    "    \n",
    "    # 输出响应，以确认操作结果\n",
    "    print(\"print delete operation result ....\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "24318782-1368-4ccd-8da3-7b51d46f379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain.document_loaders import PDFMinerPDFasHTMLLoader\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "# from opensearch_vector_search import OpenSearchVectorSearch\n",
    "from opensearch_vector_search import OpenSearchVectorSearch\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain import SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import ContentHandlerBase\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.vectorstores import Zilliz\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from chinese_text_splitter import ChineseTextSplitter\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional,Any\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "import numpy as np\n",
    "sys.path.append(r\"./python\")\n",
    "from python.smart_search_dataload import *\n",
    "def new_init_knowledge_vector(self,filepath: str or List[str], \n",
    "                                   chunk_size: int=100, \n",
    "                                   chunk_overlap: int=10,\n",
    "                                   sep_word_len: int=2000,\n",
    "                                   qa_title_name: str= '',\n",
    "                                   split_to_sentence_paragraph: bool=True,\n",
    "                                   paragraph_include_sentence_num: int= 3,\n",
    "                                   text_max_length: int=350,\n",
    "                                   pdf_to_html: bool=False,\n",
    "                                   text_field: str= 'paragraph',\n",
    "                                   vector_field: str= 'sentence_vector',\n",
    "                             ):\n",
    "        loaded_files = []\n",
    "        failed_files = []\n",
    "        pdf_to_html = False if not split_to_sentence_paragraph else pdf_to_html\n",
    "        print('new....split_to_sentence_paragraph:',split_to_sentence_paragraph)\n",
    "        print('pdf_to_html:',pdf_to_html)\n",
    "        if isinstance(filepath, str):\n",
    "            if not os.path.exists(filepath):\n",
    "                print(\"Path does not exist\")\n",
    "                return None\n",
    "            elif os.path.isfile(filepath):\n",
    "                file = os.path.split(filepath)[-1]\n",
    "                try:\n",
    "                    docs = load_file(filepath,self.language,pdf_to_html,chunk_size)\n",
    "                    print(f\"{file} Loaded successfully\")\n",
    "                    loaded_files.append(filepath)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print(f\"{file} Failed to load\")\n",
    "                    return None\n",
    "            elif os.path.isdir(filepath):\n",
    "                \n",
    "                docs = []\n",
    "                for file in tqdm(os.listdir(filepath), desc=\"Load the file\"):\n",
    "                    fullfilepath = os.path.join(filepath, file)\n",
    "                    try:\n",
    "                        doc = load_file(fullfilepath,self.language,pdf_to_html,chunk_size)\n",
    "                        docs += doc\n",
    "                        loaded_files.append(fullfilepath)\n",
    "                    except Exception as e:\n",
    "                        failed_files.append(file)\n",
    "\n",
    "                if len(failed_files) > 0:\n",
    "                    print(\"The following files failed to load:\")\n",
    "                    for file in failed_files:\n",
    "                        print(file,end=\"\\n\")\n",
    "        else:\n",
    "            docs = []\n",
    "            for file in filepath:\n",
    "                try:\n",
    "                    print(\"begin to load file, file:\",file,self.language)\n",
    "                    docs = load_file(file,self.language,pdf_to_html,chunk_size)\n",
    "                    print(f\"{file} Loaded successfully\")\n",
    "                    loaded_files.append(file)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print(f\"{file} Failed to load\")\n",
    "        \n",
    "        if len(docs) > 0:\n",
    "            print(\"The file is loaded and the vector library is being generated\")            \n",
    "            if self.vector_store is not None:\n",
    "                texts = [d.page_content for d in docs]\n",
    "                metadatas = [d.metadata for d in docs]\n",
    "                if split_to_sentence_paragraph:\n",
    "                    new_texts = []\n",
    "                    new_metadatas = []\n",
    "                    if len(metadatas) > 0 and 'row' in metadatas[0].keys():\n",
    "                        new_texts,new_metadatas = csv_processor(texts,metadatas,self.language,qa_title_name,sep_word_len,self.embedding_type,text_max_length=text_max_length)\n",
    "                    elif len(texts) > 0 and texts[0].find('<html>') >=0:\n",
    "                        new_texts,new_metadatas = html_file_processor(docs[0],self.language,self.embedding_type,text_max_length)\n",
    "                        # print('new_texts:',new_texts)\n",
    "                        # print('new_metadatas:',new_metadatas)\n",
    "                    else:\n",
    "                        texts_length = len(texts)\n",
    "                        for i in range(0, texts_length):\n",
    "                            paragraph = assemble_paragraph(texts,i,paragraph_include_sentence_num)\n",
    "                            new_texts.append(paragraph)\n",
    "                            metadata = metadatas[i]\n",
    "                            metadata['sentence'] = truncate_text(texts[i],text_max_length) if self.embedding_type=='sagemaker' else texts[i]\n",
    "                            new_metadatas.append(metadata)\n",
    "                    if len(metadatas)>0:\n",
    "                        deleteOriginal(self.vector_store.client,index,metadatas[0][\"source\"])\n",
    "                    ids = self.vector_store.add_texts_sentence_in_metadata(new_texts, new_metadatas, bulk_size=bulk_size, text_field=text_field,vector_field=vector_field,embedding_type=self.embedding_type)\n",
    "                else:\n",
    "                    if len(metadatas)>0:\n",
    "                        deleteOriginal(self.vector_store.client,index,metadatas[0][\"source\"])\n",
    "                    new_texts = [truncate_text(text,text_max_length) for text in texts] if self.embedding_type=='sagemaker' else texts\n",
    "                    ids = self.vector_store.add_texts(texts, metadatas, bulk_size=bulk_size,text_field=text_field,vector_field=vector_field,embedding_type=self.embedding_type)\n",
    "                return loaded_files\n",
    "            else:\n",
    "                print(\"Vector library is not specified, please specify the vector database\")\n",
    "        else:\n",
    "            print(\"None of the files loaded successfully, please check the file to upload again.\")\n",
    "            return loaded_files\n",
    "SmartSearchDataload.init_knowledge_vector=new_init_knowledge_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f5e5bd59-49c9-4255-b37f-f16d2d9d151d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init opensearch vector store\n",
      "new....split_to_sentence_paragraph: True\n",
      "pdf_to_html: False\n",
      "begin to load ../docs/亚马逊云计算服务.docx file\n",
      "begin load and split\n",
      "亚马逊云计算服务.docx Loaded successfully\n",
      "The file is loaded and the vector library is being generated\n",
      "begin to delete original\n",
      "print delete operation result ....\n",
      "{'took': 40, 'timed_out': False, 'total': 37, 'deleted': 37, 'batches': 1, 'version_conflicts': 0, 'noops': 0, 'retries': {'bulk': 0, 'search': 0}, 'throttled_millis': 0, 'requests_per_second': -1.0, 'throttled_until_millis': 0, 'failures': []}\n",
      "1  Complete the import of the following documents: ['../docs/亚马逊云计算服务.docx']  File import takes time: 0:00:04.521961\n"
     ]
    }
   ],
   "source": [
    "#sys.path.append(r\"./python\")\n",
    "\n",
    "search_qa = SmartSearchDataload()\n",
    "search_qa.init_cfg(index,\n",
    "                 username,\n",
    "                 password,\n",
    "                 host,\n",
    "                 port,\n",
    "                 region,\n",
    "                 embedding_endpoint_name,\n",
    "                 language=language\n",
    "                 )\n",
    "files = os.listdir(local_path)\n",
    "i = 0\n",
    "for file in files:\n",
    "    local_file = local_path+file\n",
    "    now1 = datetime.now()#begin time\n",
    "    loaded_files = search_qa.init_knowledge_vector(local_file,\n",
    "                                                   chunk_size,\n",
    "                                                   chunk_overlap,\n",
    "                                                   sep_word_len,\n",
    "                                                   qa_title_name,\n",
    "                                                   split_to_sentence_paragraph,\n",
    "                                                   paragraph_include_sentence_num,\n",
    "                                                   text_max_length,\n",
    "                                                   pdf_to_html,\n",
    "                                                   text_field,\n",
    "                                                   vector_field)\n",
    "    now2 = datetime.now()#endtime\n",
    "    i += 1\n",
    "    print(i,\" Complete the import of the following documents:\", str(loaded_files),\" File import takes time:\",now2-now1 )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
